<style>
r { color: Red }
o { color: Orange }
g { color: Green }
c { color: Cyan }
blue { color: Blue }
customb { color: #006699 }
</style>

# Welcome!
I am a fourth-year Ph.D. candidate in Computer Science at the University of Illinois at Urbana-Champaign advised by Prof. Julia Hockenmaier. My research primarily centers around: 
- Improving our understanding of the inner workings of LLMs (e.g., mechanistic interpretability).
- Applying SAEs for practical purposes.
- Creating and analyzing novel deep learning model architectures (e.g., mixture-of-experts).

## Education
- University of Illinois at Urbana-Champaign &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Aug 2021 ~ Current
  - Ph.D. student in Computer Science								       		
- Seoul National University  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mar 2013 ~ Feb 2021
  - B.S. in Electrical and Computer Enginering

## Fellowships and Awards
- CS Ph.D. Fellowship (UIUC)
  - Sep 2023 - May 2024
  - Sep 2024 - May 2025
- Conference Presentation Awards for Graduate Students (UIUC)
  - FA24
- Excellent TA Award (UIUC)
  - FA23, FA24

## Publications
1. Anonymous <br><customb>Under Review at COLM 2025</customb><br><ins>Ikhyun Cho</ins> and Julia Hockenmaier<br><br>
2. Anonymous <br><customb>Under Review at ACL 2025</customb><br><ins>Ikhyun Cho</ins>, Gaeul Kwon, and Julia Hockenmaier<br><br>
3. The Power of Bullet Lists: Reducing Mistakes in Large Language Models with a Simple Primer <br><customb>NAACL 2025 Findings</customb><br><ins>Ikhyun Cho</ins>, Changyeon Park, and Julia Hockenmaier<br><br>
4. Tutor-ICL: Guiding Large Language Models for Improved In-Context Learning Performance <br><a href="https://aclanthology.org/2024.findings-emnlp.554/" style="color: #006699;">EMNLP 2024 Findings</a><br><ins>Ikhyun Cho</ins>, Gaeul Kwon, and Julia Hockenmaier<br><br>
5. SIR-ABSC: Incorporating Syntax into RoBERTa-based Sentiment Analysis Models with a Special Aggregator Token<br><a href="https://aclanthology.org/2023.findings-emnlp.572/" style="color: #006699;">EMNLP 2023 Findings</a><br><ins>Ikhyun Cho</ins>, Yoonhwa Jung, and Julia Hockenmaier<br><br>
6. VisualSiteDiary: A Detector-Free Vision-Language Transformer Model for Captioning Photologs for Daily Construction Reporting and Image Retrievals<br><a href="https://www.sciencedirect.com/science/article/pii/S092658052400219X" style="color: #006699;">Elsevier 2024: Automation in Construction</a><br>Yoonhwa Jung, <ins>Ikhyun Cho</ins>, and Julia Hockenmaier<br><br>
7. Pea-KD: Parameter-efficient and accurate Knowledge Distillation on BERT<br><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0263592" style="color: #006699;">PLOS ONE 2022</a><br><ins>Ikhyun Cho</ins> and U Kang<br><br>
8. SensiMix: Sensitivity-Aware 8-bit index & 1-bit value mixed precision quantization for BERT compression<br><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0265621" style="color: #006699;">PLOS ONE 2022</a><br>Tairen Piao, <ins>Ikhyun Cho</ins>, and U Kang

## Unfortunate Publications ðŸ˜­
1. Duplicate-and-Share: A Novel Approach to Efficient Vision Transformer Unlearning<br><ins>Ikhyun Cho</ins>, Changyeon Park, and Julia Hockenmaier<br><br>
2. Prompting for Mixture-of-Experts: A Prompt-based Mixture-of-Experts framework for Stylized Image Captioning<br><ins>Ikhyun Cho</ins>, Yoonhwa Jung, and Julia Hockenmaier<br><br>
